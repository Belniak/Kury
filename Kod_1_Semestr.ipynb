{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39CDFWTJXt8b",
        "outputId": "3ad83a13-2e3b-4246-a9ee-56caf5ff455f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wykorzystane rasy: ['leghorn' 'newhampshire' 'plymouthrock' 'silkie' 'sussex']\n",
            "Found 160 validated image filenames belonging to 5 classes.\n",
            "Found 40 validated image filenames belonging to 5 classes.\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 45s 15s/step - loss: 0.6440 - accuracy: 0.7875 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 37s 10s/step - loss: 0.0793 - accuracy: 0.9688 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 37s 14s/step - loss: 0.0221 - accuracy: 0.9875 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 34s 11s/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 39s 13s/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from math import ceil\n",
        "\n",
        "# Load the labels\n",
        "df_labels = pd.read_csv(\"/content/drive/MyDrive/Kury/breeds.csv\", sep=\";\")\n",
        "train_file = '/content/drive/MyDrive/Kury/train/'\n",
        "\n",
        "# Number of breeds to use\n",
        "num_breeds = 5\n",
        "im_size = 224\n",
        "batch_size = 64\n",
        "\n",
        "# Get the top 'num_breeds' breeds\n",
        "breed_counts = df_labels['breed'].value_counts()\n",
        "top_breeds = breed_counts.nlargest(num_breeds).index\n",
        "\n",
        "# Filter the DataFrame to only include images of the top breeds\n",
        "df_labels = df_labels[df_labels['breed'].isin(top_breeds)]\n",
        "\n",
        "# Create the image file names\n",
        "df_labels['img_file'] = df_labels['id'].apply(lambda x: x + \".jpg\")\n",
        "\n",
        "print(\"Wykorzystane rasy:\", df_labels['breed'].unique())\n",
        "\n",
        "# Encode the breed labels\n",
        "encoder = LabelEncoder()\n",
        "df_labels['breed'] = encoder.fit_transform(df_labels['breed'])\n",
        "\n",
        "# Convert labels to strings to match the requirement for sparse class mode\n",
        "df_labels['breed'] = df_labels['breed'].astype(str)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df_labels, test_size=0.2, stratify=df_labels['breed'])\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, horizontal_flip=True, vertical_flip=True, rotation_range=20)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    directory=train_file,\n",
        "    x_col='img_file',\n",
        "    y_col='breed',\n",
        "    target_size=(im_size, im_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    directory=train_file,\n",
        "    x_col='img_file',\n",
        "    y_col='breed',\n",
        "    target_size=(im_size, im_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "# Load the ResNet50V2 model, excluding the top layer, and add custom layers\n",
        "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Dense(num_breeds, activation='softmax')(x)\n",
        "\n",
        "# Combine the base model and the custom layers into a new model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "epochs = 5\n",
        "learning_rate = 1e-3\n",
        "optimizer = RMSprop(learning_rate=learning_rate, rho=0.9)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "# Calculate steps_per_epoch and validation_steps\n",
        "steps_per_epoch = ceil(train_generator.samples / batch_size)\n",
        "validation_steps = ceil(test_generator.samples / batch_size)\n",
        "\n",
        "# Train the model\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=validation_steps\n",
        ")\n",
        "\n",
        "# Save the model for future predictions\n",
        "model.save(\"model\")\n"
      ]
    }
  ]
}